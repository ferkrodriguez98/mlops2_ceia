# Streaming de datos en tiempo real con FastAPI, Redis y MLflow

## 1. IntroducciÃ³n
Este proyecto implementa un **sistema de inferencia en tiempo real** basado en una arquitectura distribuida compuesta por **FastAPI**, **Redis** y **MLflow**. Su objetivo es procesar flujos continuos de datos generados de manera sintÃ©tica, enviarlos a travÃ©s de un sistema de colas y obtener predicciones de modelos de machine learning registrados en MLflow.

El enfoque permite desacoplar la recepciÃ³n de datos (API), el almacenamiento temporal (Redis Streams), el procesamiento asÃ­ncrono (Worker) y la gestiÃ³n de modelos (MLflow), logrando un pipeline modular, escalable y fÃ¡cilmente extensible.

---

## 2. Arquitectura general

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI       â”‚ ---> â”‚  Redis Stream  â”‚ ---> â”‚  Stream Worker (ML)   â”‚ ---> â”‚  MLflow Server  â”‚
â”‚ (API Gateway)  â”‚      â”‚  (Broker)      â”‚      â”‚  (Inferencia AsÃ­ncr.) â”‚      â”‚ (Model Registry)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                        â”‚                          â”‚                            â”‚
       â–¼                        â–¼                          â–¼                            â–¼
   Usuario / Cliente     Mensajes JSON             Predicciones ML              Almacenamiento Modelo
```

---

## 3. Componentes del sistema

### ğŸ”¹ FastAPI (`fastapi`)
- Expone endpoints REST para recibir datos (`/predict_stream`).
- Publica los mensajes en **Redis Streams** bajo la clave `mlops_stream`.
- ActÃºa como punto de entrada al sistema.

### ğŸ”¹ Redis (`mlops2_ceia-redis-1`)
- Gestiona la cola de mensajes mediante **Streams**.
- Permite crear **grupos de consumidores** (e.g. `workers`), distribuyendo la carga de manera eficiente.

### ğŸ”¹ Stream Worker (`mlops2_ceia-stream_worker`)
- Consume los mensajes desde Redis.
- Carga los modelos desde MLflow (`SVC`, `Knn_Classifier`, `LightGBM_Classifier`).
- Realiza la inferencia y genera predicciones en tiempo real.
- Guarda los resultados en `predicciones.csv` y muestra logs de ejecuciÃ³n.

### ğŸ”¹ Fake Data Producer (`mlops2_ceia-fake_data_producer`)
- Genera datos sintÃ©ticos en base a estadÃ­sticas reales del dataset `enriched_employee_dataset.csv`.
- EnvÃ­a registros simulados al endpoint `/predict_stream`.

### ğŸ”¹ MLflow (`mlflow`)
- Servidor de modelos (puerto 5001).
- Gestiona versiones y dependencias de modelos entrenados.

---

## 4. Flujo de datos paso a paso
1. **El generador** crea observaciones sintÃ©ticas y las envÃ­a a FastAPI vÃ­a HTTP POST.
2. **FastAPI** recibe la solicitud, la empaqueta en formato JSON y la publica en el stream `mlops_stream` de Redis.
3. **Redis** almacena el mensaje temporalmente y lo asigna a un grupo de consumidores (`workers`).
4. **Stream Worker** recupera el mensaje, identifica el modelo solicitado (e.g. `svm`, `knn`, `lightgbm`), y solicita el modelo correspondiente a MLflow.
5. **MLflow** devuelve el modelo registrado (Ãºltima versiÃ³n estable) y el worker realiza la inferencia.
6. **El resultado** se imprime en logs y se almacena en un archivo CSV con timestamp y predicciÃ³n.

---

## 5. EjecuciÃ³n con Docker Compose

### ğŸ”§ Levantar todos los servicios
```bash
docker compose up --build
```

### ğŸ“Š Verificar contenedores activos
```bash
docker ps
```

### ğŸ” Consultar logs del Worker
```bash
docker logs -f mlops2_ceia-stream_worker-1
```

### ğŸ§  Probar el flujo de datos manualmente
Enviar datos sintÃ©ticos a la API FastAPI:
```bash
curl -X POST "http://localhost:8800/predict_stream?model=svm" \
     -H "Content-Type: application/json" \
     -d '{"data": [{"Designation": 1.5, "Resource_Allocation": 1.7, "Sleep_Hours": 7}]}'
```

---

## 6. Logs reales de inferencia

**Worker** (conectado a Redis y MLflow):
```
ğŸ“¥ Mensaje recibido 1760296007925-0 | Modelo: knn
[INFO] Loading model from URI: models:/Knn_Classifier/1
âŒ Error procesando mensaje: The feature names should match those that were passed during fit.

ğŸ“¥ Mensaje recibido 1760296014078-0 | Modelo: lightgbm
[INFO] Loading model from URI: models:/LightGBM_Classifier/1
âœ… PredicciÃ³n completada (1 muestras): [2]
```

---

## 7. ValidaciÃ³n y monitoreo
- El **worker** imprime las predicciones directamente en consola.
- El archivo `predicciones.csv` almacena resultados persistentes para auditorÃ­a.
- Se puede inspeccionar el stream desde Redis CLI:
  ```bash
  docker exec -it mlops2_ceia-redis-1 redis-cli
  xlen mlops_stream
  ```

---

## 8. Conclusiones
- El sistema permite procesar flujos de datos en **tiempo real**, manteniendo el desacoplamiento entre los mÃ³dulos de entrada, almacenamiento, procesamiento e inferencia.
- Su estructura facilita la **escalabilidad horizontal**, permitiendo mÃºltiples workers concurrentes.
- Redis Streams ofrece un mecanismo robusto para **gestionar eventos** y **garantizar entrega**.
- MLflow centraliza la gestiÃ³n de modelos y versiones, permitiendo replicar experimentos y mantener control sobre las dependencias.

Este enfoque constituye una base sÃ³lida para sistemas de **MLOps con inferencia en streaming**, aplicables tanto a datos sintÃ©ticos como a entornos de producciÃ³n reales.

